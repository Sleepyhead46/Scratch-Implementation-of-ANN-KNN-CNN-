{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1b6c4333-2d2a-4093-ac2c-c8aafc571326",
      "metadata": {
        "id": "1b6c4333-2d2a-4093-ac2c-c8aafc571326"
      },
      "source": [
        "#### Import essential libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5c5cb191-e239-4365-8af0-5bea6a6b8708",
      "metadata": {
        "id": "5c5cb191-e239-4365-8af0-5bea6a6b8708",
        "tags": [
          "hide-input"
        ]
      },
      "outputs": [],
      "source": [
        "# numpy for linear algebra\n",
        "import numpy as np\n",
        "\n",
        "# matplotlib for plotting the loss functions and/or accuracy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# accuracy score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# show progress bar\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ef3240a-94de-472f-8ced-8def31692912",
      "metadata": {
        "id": "0ef3240a-94de-472f-8ced-8def31692912"
      },
      "source": [
        "#### [Activation class](https://pythonandml.github.io/dlbook/content/multilayer_perceptrons/activation.html)\n",
        "\n",
        "This class will contain class methods to calculate activation functions and also it will calculate the forward propagation and backpropagation as per the decsription in the chapter [Shortcut to calculate forward pass and backpropagation across layers](https://pythonandml.github.io/dlbook/content/multilayer_perceptrons/shortcut_to_calculate_forward_back_propagation.html) (link to previous chapter)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c3a83947-a79e-425f-87cc-e692cad8862a",
      "metadata": {
        "id": "c3a83947-a79e-425f-87cc-e692cad8862a",
        "tags": [
          "hide-input"
        ]
      },
      "outputs": [],
      "source": [
        "class Activation:\n",
        "\n",
        "    def __init__(self, activation_type=None):\n",
        "        '''\n",
        "        Parameters\n",
        "\n",
        "        activation_type: type of activation\n",
        "        available options are 'sigmoid', 'linear', 'tanh', 'softmax', 'prelu' and 'relu'\n",
        "        '''\n",
        "        if activation_type is None:\n",
        "            self.activation_type = 'linear'\n",
        "        else:\n",
        "            self.activation_type = activation_type\n",
        "\n",
        "    def linear(self, x):\n",
        "        '''\n",
        "        Parameters\n",
        "\n",
        "        x: input matrix of shape (m, d)\n",
        "        where 'm' is the number of samples (in case of batch gradient descent of size m)\n",
        "        and 'd' is the number of features\n",
        "        '''\n",
        "        return x\n",
        "\n",
        "    def d_linear(self, x):\n",
        "        '''\n",
        "        Parameters\n",
        "\n",
        "        x: input matrix of shape (m, d)\n",
        "        where 'm' is the number of samples (in case of batch gradient descent of size m)\n",
        "        and 'd' is the number of features\n",
        "        '''\n",
        "        return np.ones(x.shape)\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        '''\n",
        "        Parameters\n",
        "\n",
        "        x: input matrix of shape (m, d)\n",
        "        where 'm' is the number of samples (in case of batch gradient descent of size m)\n",
        "        and 'd' is the number of features\n",
        "        '''\n",
        "        return 1/(1+np.exp(-x))\n",
        "\n",
        "    def d_sigmoid(self, x):\n",
        "        '''\n",
        "        Parameters\n",
        "\n",
        "        x: input matrix of shape (m, d)\n",
        "        where 'm' is the number of samples (in case of batch gradient descent of size m)\n",
        "        and 'd' is the number of features\n",
        "        '''\n",
        "        return self.sigmoid(x) * (1-self.sigmoid(x))\n",
        "\n",
        "    def tanh(self, x):\n",
        "        '''\n",
        "        Parameters\n",
        "\n",
        "        x: input matrix of shape (m, d)\n",
        "        where 'm' is the number of samples (in case of batch gradient descent of size m)\n",
        "        and 'd' is the number of features\n",
        "        '''\n",
        "        return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
        "\n",
        "    def d_tanh(self, x):\n",
        "        '''\n",
        "        Parameters\n",
        "\n",
        "        x: input matrix of shape (m, d)\n",
        "        where 'm' is the number of samples (in case of batch gradient descent of size m)\n",
        "        and 'd' is the number of features\n",
        "        '''\n",
        "        return 1-(self.tanh(x))**2\n",
        "\n",
        "    def ReLU(self, x):\n",
        "        '''\n",
        "        Parameters\n",
        "\n",
        "        x: input matrix of shape (m, d)\n",
        "        where 'm' is the number of samples (in case of batch gradient descent of size m)\n",
        "        and 'd' is the number of features\n",
        "        '''\n",
        "        return x * (x > 0)\n",
        "\n",
        "    def d_ReLU(self, x):\n",
        "        '''\n",
        "        Parameters\n",
        "\n",
        "        x: input matrix of shape (m, d)\n",
        "        where 'm' is the number of samples (in case of batch gradient descent of size m)\n",
        "        and 'd' is the number of features\n",
        "        '''\n",
        "        return (x>0)*np.ones(x.shape)\n",
        "\n",
        "    def PReLU(self, x, alpha=0.2):\n",
        "        '''\n",
        "        Parameters\n",
        "        alpha: slope parameter (ùõº)\n",
        "\n",
        "        x: input matrix of shape (m, d)\n",
        "        where 'm' is the number of samples (or rows)\n",
        "        and 'd' is the number of features (or columns)\n",
        "        '''\n",
        "        return np.where(x > 0, x, alpha*x)\n",
        "\n",
        "    def d_PReLU(self, x, alpha=0.2):\n",
        "        '''\n",
        "        Parameters\n",
        "        alpha: slope parameter (ùõº)\n",
        "\n",
        "        x: input matrix of shape (m, d)\n",
        "        where 'm' is the number of samples (or rows)\n",
        "        and 'd' is the number of features (or columns)\n",
        "        '''\n",
        "        return np.where(x > 0, 1, alpha)\n",
        "\n",
        "    def softmax(self, x):\n",
        "        '''\n",
        "        Parameters\n",
        "\n",
        "        x: input matrix of shape (m, d)\n",
        "        where 'm' is the number of samples (in case of batch gradient descent of size m)\n",
        "        and 'd' is the number of features\n",
        "        '''\n",
        "        z = x - np.max(x, axis=-1, keepdims=True)\n",
        "        numerator = np.exp(z)\n",
        "        denominator = np.sum(numerator, axis=-1, keepdims=True)\n",
        "        softmax = numerator / denominator\n",
        "        return softmax\n",
        "\n",
        "    def d_softmax(self, x):\n",
        "        '''\n",
        "        Parameters\n",
        "\n",
        "        x: input matrix of shape (m, d)\n",
        "        where 'm' is the number of samples (in case of batch gradient descent of size m)\n",
        "        and 'd' is the number of features\n",
        "        '''\n",
        "        if len(x.shape)==1:\n",
        "            x = np.array(x).reshape(1,-1)\n",
        "        else:\n",
        "            x = np.array(x)\n",
        "        m, d = x.shape\n",
        "        a = self.softmax(x)\n",
        "        tensor1 = np.einsum('ij,ik->ijk', a, a)\n",
        "        tensor2 = np.einsum('ij,jk->ijk', a, np.eye(d, d))\n",
        "        return tensor2 - tensor1\n",
        "\n",
        "    def get_activation(self, x):\n",
        "        '''\n",
        "        Parameters\n",
        "\n",
        "        x: input matrix of shape (m, d)\n",
        "        where 'm' is the number of samples (in case of batch gradient descent of size m)\n",
        "        and 'd' is the number of features\n",
        "        '''\n",
        "        if self.activation_type == 'sigmoid':\n",
        "            return self.sigmoid(x)\n",
        "        elif self.activation_type == 'tanh':\n",
        "            return self.tanh(x)\n",
        "        elif self.activation_type == 'relu':\n",
        "            return self.ReLU(x)\n",
        "        elif self.activation_type == 'linear':\n",
        "            return self.linear(x)\n",
        "        elif self.activation_type == 'prelu':\n",
        "            return self.PReLU(x)\n",
        "        elif self.activation_type == 'softmax':\n",
        "            return self.softmax(x)\n",
        "        else:\n",
        "            raise ValueError(\"Valid Activations are only 'sigmoid', 'linear', 'tanh' 'softmax', 'prelu' and 'relu'\")\n",
        "\n",
        "    def get_d_activation(self, x):\n",
        "        '''\n",
        "        Parameters\n",
        "\n",
        "        x: input matrix of shape (m, d)\n",
        "        where 'm' is the number of samples (in case of batch gradient descent of size m)\n",
        "        and 'd' is the number of features\n",
        "        '''\n",
        "        if self.activation_type == 'sigmoid':\n",
        "            return self.d_sigmoid(x)\n",
        "        elif self.activation_type == 'tanh':\n",
        "            return self.d_tanh(x)\n",
        "        elif self.activation_type == 'relu':\n",
        "            return self.d_ReLU(x)\n",
        "        elif self.activation_type == 'linear':\n",
        "            return self.d_linear(x)\n",
        "        elif self.activation_type == 'prelu':\n",
        "            return self.d_PReLU(x)\n",
        "        elif self.activation_type == 'softmax':\n",
        "            return self.d_softmax(x)\n",
        "        else:\n",
        "            raise ValueError(\"Valid Activations are only 'sigmoid', 'linear', 'tanh', 'softmax', 'prelu' and 'relu'\")\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.X = X\n",
        "        z = self.get_activation(X)\n",
        "        return z\n",
        "\n",
        "    def backpropagation(self, dz):\n",
        "        f_prime = self.get_d_activation(self.X)\n",
        "        if self.activation_type=='softmax':\n",
        "            # because derivative of softmax is a tensor\n",
        "            dx = np.einsum('ijk,ik->ij', f_prime, dz)\n",
        "        else:\n",
        "            dx = dz * f_prime\n",
        "        return dx"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "774d77ff-8791-4905-876a-2d7d8ea92223",
      "metadata": {
        "id": "774d77ff-8791-4905-876a-2d7d8ea92223"
      },
      "source": [
        "#### [Cost function](https://pythonandml.github.io/dlbook/content/multilayer_perceptrons/cost_functions.html)\n",
        "\n",
        "Follow the lecture to develop the cost function class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a7020c13-65bf-489f-a81a-ed31ab7c768a",
      "metadata": {
        "id": "a7020c13-65bf-489f-a81a-ed31ab7c768a",
        "tags": [
          "hide-input"
        ]
      },
      "outputs": [],
      "source": [
        "class Cost:\n",
        "\n",
        "    def __init__(self, cost_type='mse'):\n",
        "        '''\n",
        "        Parameters\n",
        "\n",
        "        cost_type: type of cost function\n",
        "        available options are 'mse', and 'cross-entropy'\n",
        "        '''\n",
        "        self.cost_type = cost_type\n",
        "\n",
        "    def mse(self, a, y):\n",
        "        '''\n",
        "        Parameters\n",
        "\n",
        "        a: Predicted output array of shape (m, d)\n",
        "        y: Actual output array of shape (m, d)\n",
        "        '''\n",
        "        return (1/2)*np.sum((np.linalg.norm(a-y, axis=1))**2)\n",
        "\n",
        "    def d_mse(self, a, y):\n",
        "        '''\n",
        "        represents dJ/da\n",
        "\n",
        "        Parameters\n",
        "\n",
        "        a: Predicted output array of shape (m, d)\n",
        "        y: Actual output array of shape (m, d)\n",
        "        '''\n",
        "        return a - y\n",
        "\n",
        "    def cross_entropy(self, a, y, epsilon=1e-12):\n",
        "        '''\n",
        "        Parameters\n",
        "\n",
        "        a: Predicted output array of shape (m, d)\n",
        "        y: Actual output array of shape (m, d)\n",
        "        '''\n",
        "        a = np.clip(a, epsilon, 1. - epsilon)\n",
        "        return -np.sum(y*np.log(a))\n",
        "\n",
        "    def d_cross_entropy(self, a, y, epsilon=1e-12):\n",
        "        '''\n",
        "        represents dJ/da\n",
        "\n",
        "        Parameters\n",
        "\n",
        "        a: Predicted output array of shape (m, d)\n",
        "        y: Actual output array of shape (m, d)\n",
        "        '''\n",
        "        a = np.clip(a, epsilon, 1. - epsilon)\n",
        "        return -y/a\n",
        "\n",
        "    def get_cost(self, a, y):\n",
        "        '''\n",
        "        Parameters\n",
        "\n",
        "        a: Predicted output array of shape (m, d)\n",
        "        y: Actual output array of shape (m, d)\n",
        "        '''\n",
        "        if self.cost_type == 'mse':\n",
        "            return self.mse(a, y)\n",
        "        elif self.cost_type == 'cross-entropy':\n",
        "            return self.cross_entropy(a, y)\n",
        "        else:\n",
        "            raise ValueError(\"Valid cost functions are only 'mse', and 'cross-entropy'\")\n",
        "\n",
        "    def get_d_cost(self, a, y):\n",
        "        '''\n",
        "        Parameters\n",
        "\n",
        "        a: Predicted output array of shape (m, d)\n",
        "        y: Actual output array of shape (m, d)\n",
        "        '''\n",
        "        if self.cost_type == 'mse':\n",
        "            return self.d_mse(a, y)\n",
        "        elif self.cost_type == 'cross-entropy':\n",
        "            return self.d_cross_entropy(a, y)\n",
        "        else:\n",
        "            raise ValueError(\"Valid cost functions are only 'mse', and 'cross-entropy'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b84e00ad-6a32-4b22-a7b1-8faaf747e3a4",
      "metadata": {
        "id": "b84e00ad-6a32-4b22-a7b1-8faaf747e3a4"
      },
      "source": [
        "#### [Optimizers](https://pythonandml.github.io/dlbook/content/multilayer_perceptrons/gradient_descent.html)\n",
        "\n",
        "This class contains different optimizers (such as RMSProp, Adam, etc) used for updating the parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4897adb5-b894-42b2-aea0-51111997b091",
      "metadata": {
        "id": "4897adb5-b894-42b2-aea0-51111997b091",
        "tags": [
          "hide-input"
        ]
      },
      "outputs": [],
      "source": [
        "class Optimizer:\n",
        "\n",
        "    def __init__(self, optimizer_type=None, shape_W=None, shape_b=None,\n",
        "                 momentum1=0.9, momentum2=0.999, epsilon=1e-8):\n",
        "        '''\n",
        "        Parameters\n",
        "\n",
        "        momentum1: float hyperparameter >= 0 that accelerates gradient descent in the relevant\n",
        "                   direction and dampens oscillations. Defaults to 0, i.e., vanilla gradient descent.\n",
        "                   Also used in RMSProp\n",
        "        momentum2: used in Adam only\n",
        "        optimizer_type: type of optimizer\n",
        "                        available options are 'gd', 'sgd' (This also includes momentum), 'adam', and 'rmsprop'\n",
        "        shape_W: Shape of the weight matrix W/ Kernel K\n",
        "        shape_b: Shape of the bias matrix b\n",
        "        epsilon: parameter used in RMSProp and Adam to avoid division by zero error\n",
        "        '''\n",
        "\n",
        "        if optimizer_type is None:\n",
        "            self.optimizer_type = 'adam'\n",
        "        else:\n",
        "            self.optimizer_type = optimizer_type\n",
        "\n",
        "        self.momentum1 = momentum1\n",
        "        self.momentum2 = momentum2\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "        self.vdW = np.zeros(shape_W)\n",
        "        self.vdb = np.zeros(shape_b)\n",
        "\n",
        "        self.SdW = np.zeros(shape_W)\n",
        "        self.Sdb = np.zeros(shape_b)\n",
        "\n",
        "    def GD(self, dW, db, k):\n",
        "        '''\n",
        "        dW: gradient of Weight W for iteration k\n",
        "        db: gradient of bias b for iteration k\n",
        "        k: iteration number\n",
        "        '''\n",
        "        return dW, db\n",
        "\n",
        "    def SGD(self, dW, db, k):\n",
        "        '''\n",
        "        dW: gradient of Weight W for iteration k\n",
        "        db: gradient of bias b for iteration k\n",
        "        k: iteration number\n",
        "        '''\n",
        "        self.vdW = self.momentum1*self.vdW + (1-self.momentum1)*dW\n",
        "        self.vdb = self.momentum1*self.vdb + (1-self.momentum1)*db\n",
        "\n",
        "        return self.vdW, self.vdb\n",
        "\n",
        "    def RMSProp(self, dW, db, k):\n",
        "        '''\n",
        "        dW: gradient of Weight W for iteration k\n",
        "        db: gradient of bias b for iteration k\n",
        "        k: iteration number\n",
        "        '''\n",
        "        self.SdW = self.momentum2*self.SdW + (1-self.momentum2)*(dW**2)\n",
        "        self.Sdb = self.momentum2*self.Sdb + (1-self.momentum2)*(db**2)\n",
        "\n",
        "        den_W = np.sqrt(self.SdW) + self.epsilon\n",
        "        den_b = np.sqrt(self.Sdb) + self.epsilon\n",
        "\n",
        "        return dW/den_W, db/den_b\n",
        "\n",
        "    def Adam(self, dW, db, k):\n",
        "        '''\n",
        "        dW: gradient of Weight W for iteration k\n",
        "        db: gradient of bias b for iteration k\n",
        "        k: iteration number\n",
        "        '''\n",
        "        # momentum\n",
        "        self.vdW = self.momentum1*self.vdW + (1-self.momentum1)*dW\n",
        "        self.vdb = self.momentum1*self.vdb + (1-self.momentum1)*db\n",
        "\n",
        "        # rmsprop\n",
        "        self.SdW = self.momentum2*self.SdW + (1-self.momentum2)*(dW**2)\n",
        "        self.Sdb = self.momentum2*self.Sdb + (1-self.momentum2)*(db**2)\n",
        "\n",
        "        # correction\n",
        "        if k>1:\n",
        "            vdW_h = self.vdW / (1-(self.momentum1**k))\n",
        "            vdb_h = self.vdb / (1-(self.momentum1**k))\n",
        "            SdW_h = self.SdW / (1-(self.momentum2**k))\n",
        "            Sdb_h = self.Sdb / (1-(self.momentum2**k))\n",
        "        else:\n",
        "            vdW_h = self.vdW\n",
        "            vdb_h = self.vdb\n",
        "            SdW_h = self.SdW\n",
        "            Sdb_h = self.Sdb\n",
        "\n",
        "        den_W = np.sqrt(SdW_h) + self.epsilon\n",
        "        den_b = np.sqrt(Sdb_h) + self.epsilon\n",
        "\n",
        "        return vdW_h/den_W, vdb_h/den_b\n",
        "\n",
        "    def get_optimization(self, dW, db, k):\n",
        "        if self.optimizer_type == 'gd':\n",
        "            return self.GD(dW, db, k)\n",
        "        if self.optimizer_type == 'sgd':\n",
        "            return self.SGD(dW, db, k)\n",
        "        if self.optimizer_type == 'rmsprop':\n",
        "            return self.RMSProp(dW, db, k)\n",
        "        if self.optimizer_type == 'adam':\n",
        "            return self.Adam(dW, db, k)\n",
        "        else:\n",
        "            raise ValueError(\"Valid optimizer options are only 'gd', 'sgd', 'rmsprop', and 'adam'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3182ab77-9d25-4371-8bb9-bc1a35e52eee",
      "metadata": {
        "id": "3182ab77-9d25-4371-8bb9-bc1a35e52eee"
      },
      "source": [
        "#### [Learning Rate decay](https://pythonandml.github.io/dlbook/content/multilayer_perceptrons/gradient_descent.html#learning-rate-decay)\n",
        "\n",
        "This class contains different methods to implement the learning rate decay scheduler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "bd434f94-2e43-4b8b-b475-8d7d27b657a2",
      "metadata": {
        "id": "bd434f94-2e43-4b8b-b475-8d7d27b657a2",
        "tags": [
          "hide-input"
        ]
      },
      "outputs": [],
      "source": [
        "class LearningRateDecay:\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def constant(self, t, lr_0):\n",
        "        '''\n",
        "        t: iteration\n",
        "        lr_0: initial learning rate\n",
        "        '''\n",
        "        return lr_0\n",
        "\n",
        "    def time_decay(self, t, lr_0, k):\n",
        "        '''\n",
        "        lr_0: initial learning rate\n",
        "        k: Decay rate\n",
        "        t: iteration number\n",
        "        '''\n",
        "        lr = lr_0 /(1+(k*t))\n",
        "        return lr\n",
        "\n",
        "    def step_decay(self, t, lr_0, F, D):\n",
        "        '''\n",
        "        lr_0: initial learning rate\n",
        "        F: factor value controlling the rate in which the learning date drops\n",
        "        D: ‚ÄúDrop every‚Äù iteration\n",
        "        t: current iteration\n",
        "        '''\n",
        "        mult = F**np.floor((1+t)/D)\n",
        "        lr = lr_0 * mult\n",
        "        return lr\n",
        "\n",
        "    def exponential_decay(self, t, lr_0, k):\n",
        "        '''\n",
        "        lr_0: initial learning rate\n",
        "        k: Exponential Decay rate\n",
        "        t: iteration number\n",
        "        '''\n",
        "        lr = lr_0 * np.exp(-k*t)\n",
        "        return lr"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7416475b-b38c-4bcf-98f6-11f760aa25bc",
      "metadata": {
        "id": "7416475b-b38c-4bcf-98f6-11f760aa25bc"
      },
      "source": [
        "#### [Utility function](https://pythonandml.github.io/dlbook/content/preliminaries/data_preprocessing.html)\n",
        "\n",
        "This class contains several utility functions such as one-hot vector, label encoder, normalization, etc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0daf2357-cdbf-4432-9fdd-340d1ca8de12",
      "metadata": {
        "id": "0daf2357-cdbf-4432-9fdd-340d1ca8de12",
        "tags": [
          "hide-input"
        ]
      },
      "outputs": [],
      "source": [
        "class Utility:\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def label_encoding(self, Y):\n",
        "        '''\n",
        "        Parameters:\n",
        "        Y: (m,d) shape matrix with categorical data\n",
        "        Return\n",
        "        result: label encoded data of ùëå\n",
        "        idx_list: list of the dictionaries containing the unique values\n",
        "                  of the columns and their mapping to the integer.\n",
        "        '''\n",
        "        idx_list = []\n",
        "        result = []\n",
        "        for col in range(Y.shape[1]):\n",
        "            indexes = {val: idx for idx, val in enumerate(np.unique(Y[:, col]))}\n",
        "            result.append([indexes[s] for s in Y[:, col]])\n",
        "            idx_list.append(indexes)\n",
        "        return np.array(result).T, idx_list\n",
        "\n",
        "    def onehot(self, X):\n",
        "        '''\n",
        "        Parameters:\n",
        "        X: 1D array of labels of length \"m\"\n",
        "        Return\n",
        "        X_onehot: (m,d) one hot encoded matrix (one-hot of X)\n",
        "                  (where d is the number of unique values in X)\n",
        "        indexes: dictionary containing the unique values of X and their mapping to the integer column\n",
        "        '''\n",
        "        indexes = {val: idx for idx, val in enumerate(np.unique(X))}\n",
        "        y = np.array([indexes[s] for s in X])\n",
        "        X_onehot = np.zeros((y.size, len(indexes)))\n",
        "        X_onehot[np.arange(y.size), y] = 1\n",
        "        return X_onehot, indexes\n",
        "\n",
        "    def minmax(self, X, min_X=None, max_X=None):\n",
        "        if min_X is None:\n",
        "            min_X = np.min(X, axis=0)\n",
        "        if max_X is None:\n",
        "            max_X = np.max(X, axis=0)\n",
        "        Z = (X - min_X) / (max_X - min_X)\n",
        "        return Z, min_X, max_X\n",
        "\n",
        "    def standardize(self, X, mu=None, std=None):\n",
        "        if mu is None:\n",
        "            mu = np.mean(X, axis=0)\n",
        "        if std is None:\n",
        "            std = np.std(X, axis=0)\n",
        "        Z = (X - mu) / std\n",
        "        return Z, mu, std\n",
        "\n",
        "    def inv_standardize(self, Z, mu, std):\n",
        "        X = Z*std + mu\n",
        "        return X\n",
        "\n",
        "    def train_test_split(self, X, y, test_ratio=0.2, seed=None):\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "        train_ratio = 1-test_ratio\n",
        "        indices = np.random.permutation(X.shape[0])\n",
        "        train_idx, test_idx = indices[:int(train_ratio*len(X))], indices[int(train_ratio*len(X)):]\n",
        "        X_train, X_test = X[train_idx,:], X[test_idx,:]\n",
        "        y_train, y_test = y[train_idx], y[test_idx]\n",
        "        return X_train, X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7e65c14-d669-45d4-93a7-411eb5aa73a9",
      "metadata": {
        "id": "f7e65c14-d669-45d4-93a7-411eb5aa73a9"
      },
      "source": [
        "#### [Weights initializer class](https://pythonandml.github.io/dlbook/content/multilayer_perceptrons/terminologies_part_1.html#parameter-s-initialization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3e655844-7ced-4c5d-b2d3-78879d84a555",
      "metadata": {
        "id": "3e655844-7ced-4c5d-b2d3-78879d84a555",
        "tags": [
          "hide-input"
        ]
      },
      "outputs": [],
      "source": [
        "class Weights_initializer:\n",
        "\n",
        "    def __init__(self, shape, initializer_type=None, seed=None):\n",
        "        '''\n",
        "        Parameters\n",
        "        shape: Shape of the weight matrix\n",
        "\n",
        "        initializer_type: type of weight initializer\n",
        "        available options are 'zeros', 'ones', 'random_normal', 'random_uniform',\n",
        "        'he_normal', 'xavier_normal' and 'glorot_normal'\n",
        "        '''\n",
        "        self.shape = shape\n",
        "        if initializer_type is None:\n",
        "            self.initializer_type = \"he_normal\"\n",
        "        else:\n",
        "            self.initializer_type = initializer_type\n",
        "        self.seed = seed\n",
        "\n",
        "    def zeros_initializer(self):\n",
        "        if self.seed is not None:\n",
        "            np.random.seed(self.seed)\n",
        "        return np.zeros(self.shape)\n",
        "\n",
        "    def ones_initializer(self):\n",
        "        if self.seed is not None:\n",
        "            np.random.seed(self.seed)\n",
        "        return np.ones(self.shape)\n",
        "\n",
        "    def random_normal_initializer(self):\n",
        "        if self.seed is not None:\n",
        "            np.random.seed(self.seed)\n",
        "        return np.random.normal(size=self.shape)\n",
        "\n",
        "    def random_uniform_initializer(self):\n",
        "        if self.seed is not None:\n",
        "            np.random.seed(self.seed)\n",
        "        return np.random.uniform(size=self.shape)\n",
        "\n",
        "    def he_initializer(self):\n",
        "        if self.seed is not None:\n",
        "            np.random.seed(self.seed)\n",
        "        try:\n",
        "            F, Kc, Kh, Kw = self.shape\n",
        "        except:\n",
        "            Kh, Kw = self.shape\n",
        "        return np.random.randn(*self.shape) * np.sqrt(2/Kh)\n",
        "\n",
        "    def xavier_initializer(self):\n",
        "        '''\n",
        "        shape: Shape of the Kernel matrix.\n",
        "        '''\n",
        "        if self.seed is not None:\n",
        "            np.random.seed(self.seed)\n",
        "        try:\n",
        "            F, Kc, Kh, Kw = self.shape\n",
        "        except:\n",
        "            Kh, Kw = self.shape\n",
        "        return np.random.randn(*self.shape) * np.sqrt(1/Kh)\n",
        "\n",
        "    def glorot_initializer(self):\n",
        "        '''\n",
        "        shape: Shape of the weight matrix.\n",
        "        '''\n",
        "        if self.seed is not None:\n",
        "            np.random.seed(self.seed)\n",
        "        try:\n",
        "            F, Kc, Kh, Kw = self.shape\n",
        "        except:\n",
        "            Kh, Kw = self.shape\n",
        "        return np.random.randn(*self.shape) * np.sqrt(2/(Kh+Kw))\n",
        "\n",
        "    def get_initializer(self):\n",
        "        if self.initializer_type == 'zeros':\n",
        "            return self.zeros_initializer()\n",
        "        elif self.initializer_type == 'ones':\n",
        "            return self.ones_initializer()\n",
        "        elif self.initializer_type == 'random_normal':\n",
        "            return self.random_normal_initializer()\n",
        "        elif self.initializer_type == 'random_uniform':\n",
        "            return self.random_uniform_initializer()\n",
        "        elif self.initializer_type == 'he_normal':\n",
        "            return self.he_initializer()\n",
        "        elif self.initializer_type == 'xavier_normal':\n",
        "            return self.xavier_initializer()\n",
        "        elif self.initializer_type == 'glorot_normal':\n",
        "            return self.glorot_initializer()\n",
        "        else:\n",
        "            raise ValueError(\"Valid initializer options are 'zeros', 'ones', 'random_normal', 'random_uniform', 'he_normal', 'xavier_normal', and 'glorot_normal'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37cc5276-8886-41fc-b845-f6eeb896d93b",
      "metadata": {
        "id": "37cc5276-8886-41fc-b845-f6eeb896d93b"
      },
      "source": [
        "#### Dense class\n",
        "\n",
        "Dense class implements the operation:\n",
        "\n",
        "$$\n",
        "z = XW + b^T\n",
        "$$\n",
        "\n",
        "$$\n",
        "a = f(z)\n",
        "$$\n",
        "\n",
        "where activation $f(.)$ is used if specified, else we do not use it. $W$ is a weights matrix created by the Dense layer based on [type of initialization](https://pythonandml.github.io/dlbook/content/multilayer_perceptrons/terminologies_part_1.html#parameter-s-initialization) (link to previous chapter) provided, and $b$ is a bias vector created by the layer (only applicable if use_bias is True). These are all attributes of Dense."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "b93298d4-837b-4288-b473-8491bb70fce6",
      "metadata": {
        "id": "b93298d4-837b-4288-b473-8491bb70fce6",
        "tags": [
          "hide-input"
        ]
      },
      "outputs": [],
      "source": [
        "class Dense:\n",
        "\n",
        "    def __init__(self, neurons, activation_type=None, use_bias=True,\n",
        "                 weight_initializer_type=None, weight_regularizer=None, seed=None, input_dim=None):\n",
        "\n",
        "        '''\n",
        "        Parameters:\n",
        "\n",
        "        neurons: Positive integer (number of neurons), dimensionality of the output\n",
        "\n",
        "        activation_type: type of activation\n",
        "                         available options are 'sigmoid', 'linear', 'tanh', 'softmax', 'prelu' and 'relu'\n",
        "                         If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x).\n",
        "\n",
        "        use_bias: Boolean, whether the layer uses a bias vector.\n",
        "\n",
        "        weight_initializer_type: Initializer for the kernel weights matrix.\n",
        "\n",
        "        weight_regularizer: Tuple, Regularizer function applied to the weights matrix ('L2', 0.01) or ('L1', 2)\n",
        "\n",
        "        seed: To generate reproducable results\n",
        "\n",
        "        input_dim: integer showing number of neurons in input layer\n",
        "        '''\n",
        "        self.neurons = neurons\n",
        "        self.activation = Activation(activation_type=activation_type)\n",
        "        self.use_bias = use_bias\n",
        "        self.weight_initializer_type = weight_initializer_type # none is handled\n",
        "        if weight_regularizer is None:\n",
        "            self.weight_regularizer = ('L2', 0)\n",
        "        else:\n",
        "            self.weight_regularizer = weight_regularizer\n",
        "        self.seed = seed\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "\n",
        "    def initialize_parameters(self, hl, optimizer_type):\n",
        "        '''\n",
        "        hl: Number of neurons in layer l-1\n",
        "        '''\n",
        "        shape_W = (hl, self.neurons)\n",
        "        shape_b = (self.neurons, 1)\n",
        "        initializer = Weights_initializer(shape=shape_W,\n",
        "                                          initializer_type=self.weight_initializer_type,\n",
        "                                          seed=self.seed)\n",
        "        self.W = initializer.get_initializer()\n",
        "        self.b = np.zeros(shape_b)\n",
        "\n",
        "        self.optimizer = Optimizer(optimizer_type=optimizer_type, shape_W=shape_W, shape_b=shape_b)\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.X = X\n",
        "        r = X @ self.W\n",
        "        self.z = r + self.b.T\n",
        "        a = self.activation.forward(self.z)\n",
        "        return a\n",
        "\n",
        "    def backpropagation(self, da):\n",
        "        dz = self.activation.backpropagation(da)\n",
        "        dr = dz.copy()\n",
        "        self.db = np.sum(dz, axis=0).reshape(-1,1)\n",
        "        self.dW = (self.X.T) @ dr\n",
        "        dX = dr @ (self.W.T)\n",
        "        return dX\n",
        "\n",
        "    def update(self, lr, m, k):\n",
        "        '''\n",
        "        Parameters:\n",
        "\n",
        "        lr: learning rate\n",
        "        m: batch_size (sumber of samples in batch)\n",
        "        k: iteration_number\n",
        "        '''\n",
        "        dW, db = self.optimizer.get_optimization(self.dW, self.db, k)\n",
        "\n",
        "        if self.weight_regularizer[0].lower()=='l2':\n",
        "            dW += self.weight_regularizer[1] * self.W\n",
        "        elif self.weight_regularizer[0].lower()=='l1':\n",
        "            dW += self.weight_regularizer[1] * np.sign(self.W)\n",
        "\n",
        "        self.W -= dW*(lr/m)\n",
        "        if self.use_bias:\n",
        "            self.b -= db*(lr/m)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbf3cc76-c419-492f-a2a2-cbf98008f375",
      "metadata": {
        "id": "dbf3cc76-c419-492f-a2a2-cbf98008f375"
      },
      "source": [
        "#### [Dropout class](https://pythonandml.github.io/dlbook/content/multilayer_perceptrons/dropout.html)\n",
        "\n",
        "This class will perform forward and backpropagation for a Dropout layer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8786fad9-4395-4963-9705-122a049f02d0",
      "metadata": {
        "id": "8786fad9-4395-4963-9705-122a049f02d0",
        "tags": [
          "hide-input"
        ]
      },
      "outputs": [],
      "source": [
        "class Dropout:\n",
        "\n",
        "    def __init__(self, p):\n",
        "        '''\n",
        "        Parameters\n",
        "\n",
        "        p: Dropout probability\n",
        "        '''\n",
        "        self.p = p\n",
        "        if self.p == 0:\n",
        "            self.p += 1e-6\n",
        "        if self.p == 1:\n",
        "            self.p -= 1e-6\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.mask = (np.random.rand(*X.shape) < self.p) / self.p\n",
        "        Z = X * self.mask\n",
        "        return Z\n",
        "\n",
        "    def backpropagation(self, dZ):\n",
        "        dX = dZ * self.mask\n",
        "        return dX"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e61d7c66-003d-40c5-a286-3fedac6442db",
      "metadata": {
        "id": "e61d7c66-003d-40c5-a286-3fedac6442db"
      },
      "source": [
        "#### [Batch Normalization class](https://pythonandml.github.io/dlbook/content/multilayer_perceptrons/batch_normalization.html)\n",
        "\n",
        "This class will perform forward and backpropagation for Batch Normalization layer. Right now it is only prepared for 1D data (that is it can be used only in Fully Connected or MLP layers and not in Convolution or Maxpool).\n",
        "\n",
        "> **Note:** We will initialise $\\gamma$ as ones and $\\beta$ as zeroes so that the output of the linear batch-norm transformation initially follows the standard zero-mean unit-variance normal distribution. This provides a normalised starting point, for which the model can update the $\\gamma$ and $\\beta$ to scale and shift the distribution(s) of each input accordingly (for the current layer).\n",
        "\n",
        "**Forward pass**\n",
        "\n",
        "`eps` represents: $\\epsilon$\n",
        "\n",
        "`mu` represents: $\\mu$\n",
        "\n",
        "`var` represents: $\\sigma^2$\n",
        "\n",
        "`zmu` represents: $\\bar{z_l}$\n",
        "\n",
        "`ivar` represents: $\\frac{1}{\\sqrt{\\sigma^2 + \\epsilon}}$\n",
        "\n",
        "`zhat` represents: $\\hat{z_l}$\n",
        "\n",
        "`q` represents: $q_l$\n",
        "\n",
        "**Backpropagation**\n",
        "\n",
        "This `dq` variable below represents $\\frac{\\partial J}{\\partial q_l}$\n",
        "\n",
        "`dgamma` represents: $\\frac{\\partial J}{\\partial \\gamma}$\n",
        "\n",
        "`dbeta` represents: $\\frac{\\partial J}{\\partial \\beta}$\n",
        "\n",
        "`dzhat` represents: $\\frac{\\partial J}{\\partial \\hat{z_l}}$\n",
        "\n",
        "`dvar` represents: $\\frac{\\partial J}{\\partial \\sigma^2}$\n",
        "\n",
        "`dmu` represents: $\\frac{\\partial J}{\\partial \\mu}$\n",
        "\n",
        "`dz` represents: $\\frac{\\partial J}{\\partial z_l}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "58c01495-ae8a-4997-bb17-fc6bcc199f56",
      "metadata": {
        "id": "58c01495-ae8a-4997-bb17-fc6bcc199f56",
        "tags": [
          "hide-input"
        ]
      },
      "outputs": [],
      "source": [
        "class BatchNormalization:\n",
        "\n",
        "    def __init__(self, momentum=0.9, epsilon=1e-6):\n",
        "        '''\n",
        "        Parameters\n",
        "\n",
        "        momentum: Momentum for the moving average\n",
        "        epsilon: ùúñ, Small float added to variance to avoid dividing by zero\n",
        "        '''\n",
        "        self.epsilon = epsilon\n",
        "        self.momentum = momentum\n",
        "\n",
        "    def initialize_parameters(self, d):\n",
        "        '''\n",
        "        d: Shape of input to BN layer\n",
        "        '''\n",
        "        self.gamma = np.ones((d))\n",
        "        self.beta = np.zeros((d))\n",
        "        self.running_mean = np.zeros((d))\n",
        "        self.running_var = np.zeros((d))\n",
        "\n",
        "    def forward(self, z, mode='train'):\n",
        "        '''\n",
        "        z: Input to BN layer\n",
        "        mode: forward pass used for train or test\n",
        "        '''\n",
        "        if mode=='train':\n",
        "            self.m, self.d = z.shape\n",
        "            self.mu = np.mean(z, axis = 0) # ùúá\n",
        "            self.var = np.var(z, axis=0) # ùúé^2\n",
        "            self.zmu = z - self.mu # z - ùúá\n",
        "            self.ivar = 1 / np.sqrt(self.var + self.epsilon) # ùúéùëñùëõùë£\n",
        "            self.zhat = self.zmu * self.ivar\n",
        "            q = self.gamma*self.zhat + self.beta # ql\n",
        "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * self.mu\n",
        "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * self.var\n",
        "        elif mode=='test':\n",
        "            q = (z - self.running_mean) / np.sqrt(self.running_var + self.epsilon)\n",
        "            q = self.gamma*q + self.beta\n",
        "        else:\n",
        "            raise ValueError('Invalid forward batchnorm mode \"%s\"' % mode)\n",
        "        return q\n",
        "\n",
        "    def backpropagation(self, dq):\n",
        "        self.dgamma = np.sum(dq * self.zhat, axis=0)\n",
        "        self.dbeta = np.sum(dq, axis=0)\n",
        "        dzhat = dq * self.gamma\n",
        "        dvar = np.sum(dzhat * self.zmu * (-.5) * (self.ivar**3), axis=0)\n",
        "        dmu = np.sum(dzhat * (-self.ivar), axis=0)\n",
        "        dz = dzhat * self.ivar + dvar * (2/self.m) * self.zmu + (1/self.m)*dmu\n",
        "        return dz\n",
        "\n",
        "    def update(self, lr, m, k):\n",
        "        '''\n",
        "        Parameters:\n",
        "\n",
        "        lr: learning rate\n",
        "        m: batch_size (sumber of samples in batch)\n",
        "        k: iteration_number\n",
        "        '''\n",
        "        self.gamma -= self.dgamma*(lr/m)\n",
        "        self.beta -= self.dbeta*(lr/m)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d18d3204-0701-437c-bb4e-2f043754ac6d",
      "metadata": {
        "id": "d18d3204-0701-437c-bb4e-2f043754ac6d"
      },
      "source": [
        "#### [Padding2D class](https://pythonandml.github.io/dlbook/content/convolutional_neural_networks/convolutional_layers.html)\n",
        "\n",
        "This class will perform padding on a batch of 2D image with channels. We will be considering padding as a separate layer only which contains its own forward and backpropagation operations.\n",
        "\n",
        "We need a small function such that we can extract the original input errors from the padding ones. This you can consider as **backpropagation through padding operation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "2b36dc1c-a91a-4d49-b48b-c3341b766a59",
      "metadata": {
        "id": "2b36dc1c-a91a-4d49-b48b-c3341b766a59",
        "tags": [
          "hide-input"
        ]
      },
      "outputs": [],
      "source": [
        "class Padding2D:\n",
        "\n",
        "    def __init__(self, p='valid'):\n",
        "        '''\n",
        "        Parameters:\n",
        "\n",
        "        p: padding type\n",
        "        Allowed types are only 'same', 'valid', an integer or a tuple of length 2.\n",
        "        '''\n",
        "        self.p = p\n",
        "\n",
        "    def get_dimensions(self, input_shape, kernel_size, s=(1,1)):\n",
        "        '''\n",
        "        Utility function to help get the dimension of the output after padding\n",
        "        '''\n",
        "        if len(input_shape)==4:\n",
        "            m, Nc, Nh, Nw = input_shape\n",
        "        elif len(input_shape)==3:\n",
        "            Nc, Nh, Nw = input_shape\n",
        "\n",
        "        Kh, Kw = kernel_size\n",
        "        sh, sw = s\n",
        "        p = self.p\n",
        "\n",
        "        if type(p)==int:\n",
        "            pt, pb = p, p\n",
        "            pl, pr = p, p\n",
        "\n",
        "        if type(p)==tuple:\n",
        "            ph, pw = p\n",
        "            pt, pb = ph//2, (ph+1)//2\n",
        "            pl, pr = pw//2, (pw+1)//2\n",
        "\n",
        "        elif p=='valid':\n",
        "            pt, pb = 0, 0\n",
        "            pl, pr = 0, 0\n",
        "\n",
        "        elif p=='same':\n",
        "            # calculating how much padding is required in all 4 directions\n",
        "            # (top, bottom, left and right)\n",
        "            ph = (sh-1)*Nh + Kh - sh\n",
        "            pw = (sw-1)*Nw + Kw - sw\n",
        "\n",
        "            pt, pb = ph//2, (ph+1)//2\n",
        "            pl, pr = pw//2, (pw+1)//2\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Incorrect padding type. Allowed types are only 'same', 'valid', an integer or a tuple of length 2.\")\n",
        "\n",
        "        if len(input_shape)==4:\n",
        "            output_shape = (m, Nc, Nh+pt+pb, Nw+pl+pr)\n",
        "        elif len(input_shape)==3:\n",
        "            output_shape = (Nc, Nh+pt+pb, Nw+pl+pr)\n",
        "\n",
        "        return output_shape, (pt, pb, pl, pr)\n",
        "\n",
        "    def forward(self, X, kernel_size, s=(1,1)):\n",
        "        '''\n",
        "        Parameters:\n",
        "\n",
        "        X: input of shape (m, Nc, Nh, Nw)\n",
        "\n",
        "        s: strides along height and width (sh, sw)\n",
        "\n",
        "        kernel_size: kernel size as specified in Conv2D layer\n",
        "\n",
        "        Returns:\n",
        "\n",
        "        Xp: padded X\n",
        "        '''\n",
        "        self.input_shape = X.shape\n",
        "        m, Nc, Nh, Nw = self.input_shape\n",
        "\n",
        "        self.output_shape, (self.pt, self.pb, self.pl, self.pr) = self.get_dimensions(self.input_shape,\n",
        "                                                                                      kernel_size, s=s)\n",
        "\n",
        "        zeros_r = np.zeros((m, Nc, Nh, self.pr))\n",
        "        zeros_l = np.zeros((m, Nc, Nh, self.pl))\n",
        "        zeros_t = np.zeros((m, Nc, self.pt, Nw + self.pl + self.pr))\n",
        "        zeros_b = np.zeros((m, Nc, self.pb, Nw + self.pl + self.pr))\n",
        "\n",
        "        Xp = np.concatenate((X, zeros_r), axis=3)\n",
        "        Xp = np.concatenate((zeros_l, Xp), axis=3)\n",
        "        Xp = np.concatenate((zeros_t, Xp), axis=2)\n",
        "        Xp = np.concatenate((Xp, zeros_b), axis=2)\n",
        "\n",
        "        return Xp\n",
        "\n",
        "    def backpropagation(self, dXp):\n",
        "        '''\n",
        "        Parameters:\n",
        "\n",
        "        dXp: Backprop Error of padded X (Xp)\n",
        "\n",
        "        Return:\n",
        "\n",
        "        dX: Backprop Error of X\n",
        "        '''\n",
        "        m, Nc, Nh, Nw = self.input_shape\n",
        "        dX = dXp[:, :, self.pt:self.pt+Nh, self.pl:self.pl+Nw]\n",
        "        return dX"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fec72fc0-3d72-4596-bf98-8a3e5fc61155",
      "metadata": {
        "id": "fec72fc0-3d72-4596-bf98-8a3e5fc61155"
      },
      "source": [
        "#### [Convolution2D class](https://pythonandml.github.io/dlbook/content/convolutional_neural_networks/convolutional_layers.html)\n",
        "\n",
        "This class will perform forward and backpropagation for a Convolution layer on a batch of 2D image with channels.\n",
        "\n",
        "We have an Input with batch of images (with channels) and multiple filters.\n",
        "\n",
        "Shape of Image will be $(m, N_c, N_h, N_w)$.\n",
        "\n",
        "The shape of Kernel will be $(F, K_c, K_h, K_w)$ where $F$ is the total number of filters.\n",
        "\n",
        "![](https://github.com/pythonandml/dlbook/blob/master/content/convolutional_neural_networks/images/input_batch_channels_and_filters.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73fce354-64bb-49fd-af7c-a40e45a81d53",
      "metadata": {
        "id": "73fce354-64bb-49fd-af7c-a40e45a81d53"
      },
      "source": [
        "![](https://github.com/pythonandml/dlbook/blob/master/content/convolutional_neural_networks/images/convolution_layer.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "b5c5f5cd-eea7-4208-b091-a2a29668fd80",
      "metadata": {
        "id": "b5c5f5cd-eea7-4208-b091-a2a29668fd80",
        "tags": [
          "hide-input"
        ]
      },
      "outputs": [],
      "source": [
        "class Conv2D:\n",
        "\n",
        "    def __init__(self, filters, kernel_size, s=(1, 1), p='valid',\n",
        "                 activation_type=None, use_bias=True, weight_initializer_type=None,\n",
        "                 kernel_regularizer=None, seed=None, input_shape=None):\n",
        "\n",
        "        '''\n",
        "        Parameters:\n",
        "\n",
        "        filters: Integer, the number of output filters in the convolution (F).\n",
        "\n",
        "        kernel_size: An integer or tuple/list of 2 integers,\n",
        "                     specifying the height and width of the 2D convolution window.\n",
        "\n",
        "        s: strides along height and width (sh, sw)\n",
        "\n",
        "        p: padding type\n",
        "           Allowed types are only 'same', 'valid', an integer or a tuple of length 2.\n",
        "\n",
        "        activation_type: type of activation\n",
        "                         available options are 'sigmoid', 'linear', 'tanh', 'softmax', 'prelu' and 'relu'\n",
        "                         If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x).\n",
        "\n",
        "        use_bias: Boolean, whether the layer uses a bias vector.\n",
        "\n",
        "        weight_initializer_type: Initializer for the kernel weights matrix.\n",
        "\n",
        "        kernel_regularizer: Tuple, Regularizer function applied to the kernel matrix ('L2', 0.01) or ('L1', 2)\n",
        "\n",
        "        seed: To generate reproducable results\n",
        "\n",
        "        input_shape: tuple showing size of input: (batch_size, channels, rows, cols) -> (m, Nc, Nh, Nw)\n",
        "        '''\n",
        "\n",
        "        self.padding = Padding2D(p=p)\n",
        "\n",
        "        self.F = filters\n",
        "\n",
        "        self.input_shape_x = input_shape\n",
        "\n",
        "        if type(kernel_size)==int:\n",
        "            self.kernel_size = (kernel_size, kernel_size)\n",
        "        elif type(kernel_size)==tuple and len(kernel_size)==2:\n",
        "            self.kernel_size = kernel_size\n",
        "\n",
        "        self.Kh, self.Kw  = self.kernel_size\n",
        "\n",
        "        if type(s)==int:\n",
        "            self.s = (s,s)\n",
        "        elif type(s)==tuple and len(s)==2:\n",
        "            self.s = s\n",
        "\n",
        "        self.sh, self.sw = self.s\n",
        "\n",
        "        self.activation = Activation(activation_type=activation_type)\n",
        "        self.use_bias = use_bias\n",
        "        self.weight_initializer_type = weight_initializer_type # none is handled\n",
        "        if kernel_regularizer is None:\n",
        "            self.kernel_regularizer = ('L2', 0)\n",
        "        else:\n",
        "            self.kernel_regularizer = kernel_regularizer\n",
        "        self.seed = seed\n",
        "\n",
        "    def get_dimensions(self, input_shape):\n",
        "\n",
        "        self.input_shape_x = input_shape # (3D or 4D)\n",
        "\n",
        "        # Padded X will be actual input to this Conv2D\n",
        "\n",
        "        self.input_shape, _ = self.padding.get_dimensions(self.input_shape_x,\n",
        "                                                          self.kernel_size, self.s)\n",
        "\n",
        "        if len(input_shape)==3:\n",
        "            self.Nc, self.Nh, self.Nw = self.input_shape\n",
        "        elif len(input_shape)==4:\n",
        "            self.m, self.Nc, self.Nh, self.Nw = self.input_shape\n",
        "\n",
        "        # Output shape\n",
        "        self.Oh = (self.Nh - self.Kh)//self.sh + 1\n",
        "        self.Ow = (self.Nw - self.Kw)//self.sw + 1\n",
        "\n",
        "        if len(input_shape)==3:\n",
        "            self.output_shape = (self.F, self.Oh, self.Ow)\n",
        "        elif len(input_shape)==4:\n",
        "            self.output_shape = (self.m, self.F, self.Oh, self.Ow)\n",
        "\n",
        "    def initialize_parameters(self, input_shape, optimizer_type):\n",
        "\n",
        "        self.get_dimensions(input_shape)\n",
        "\n",
        "        shape_b = (self.F, self.Oh, self.Ow)\n",
        "\n",
        "        shape_K = (self.F, self.Nc, self.Kh, self.Kw)\n",
        "\n",
        "        initializer = Weights_initializer(shape=shape_K,\n",
        "                                          initializer_type=self.weight_initializer_type,\n",
        "                                          seed=self.seed)\n",
        "\n",
        "        self.K = initializer.get_initializer()\n",
        "        self.b = np.zeros(shape_b)\n",
        "\n",
        "        self.optimizer = Optimizer(optimizer_type=optimizer_type, shape_W=shape_K, shape_b=shape_b)\n",
        "\n",
        "    def dilate2D(self, X, Dr=(1,1)):\n",
        "        dh, dw = Dr # Dilate rate\n",
        "        m, C, H, W = X.shape\n",
        "        Xd = np.insert(arr=X, obj=np.repeat(np.arange(1,W), dw-1), values=0, axis=-1)\n",
        "        Xd = np.insert(arr=Xd, obj=np.repeat(np.arange(1,H), dh-1), values=0, axis=-2)\n",
        "        return Xd\n",
        "\n",
        "    def prepare_subMatrix(self, X, Kh, Kw, s):\n",
        "        m, Nc, Nh, Nw = X.shape\n",
        "        sh, sw = s\n",
        "\n",
        "        Oh = (Nh-Kh)//sh + 1\n",
        "        Ow = (Nw-Kw)//sw + 1\n",
        "\n",
        "        strides = (Nc*Nh*Nw, Nw*Nh, Nw*sh, sw, Nw, 1)\n",
        "        strides = tuple(i * X.itemsize for i in strides)\n",
        "\n",
        "        subM = np.lib.stride_tricks.as_strided(X,\n",
        "                                               shape=(m, Nc, Oh, Ow, Kh, Kw),\n",
        "                                               strides=strides)\n",
        "\n",
        "        return subM\n",
        "\n",
        "    def convolve(self, X, K, s=(1,1), mode='front'):\n",
        "\n",
        "        F, Kc, Kh, Kw = K.shape\n",
        "        subM = self.prepare_subMatrix(X, Kh, Kw, s)\n",
        "\n",
        "        if mode=='front':\n",
        "            return np.einsum('fckl,mcijkl->mfij', K, subM)\n",
        "        elif mode=='back':\n",
        "            return np.einsum('fdkl,mcijkl->mdij', K, subM)\n",
        "        elif mode=='param':\n",
        "            return np.einsum('mfkl,mcijkl->fcij', K, subM)\n",
        "\n",
        "    def dZ_D_dX(self, dZ_D, Nh, Nw):\n",
        "\n",
        "        # Pad the dilated dZ (dZ_D -> dZ_Dp)\n",
        "\n",
        "        _, _, Hd, Wd = dZ_D.shape\n",
        "\n",
        "        ph = Nh - Hd + self.Kh - 1\n",
        "        pw = Nw - Wd + self.Kw - 1\n",
        "\n",
        "        padding_back = Padding2D(p=(ph, pw))\n",
        "\n",
        "        dZ_Dp = padding_back.forward(dZ_D, self.kernel_size, self.s)\n",
        "\n",
        "        # Rotate K by 180 degrees\n",
        "\n",
        "        K_rotated = self.K[:, :, ::-1, ::-1]\n",
        "\n",
        "        # convolve dZ_Dp with K_rotated\n",
        "\n",
        "        dXp = self.convolve(dZ_Dp, K_rotated, mode='back')\n",
        "\n",
        "        dX = self.padding.backpropagation(dXp)\n",
        "\n",
        "        return dX\n",
        "\n",
        "    def forward(self, X):\n",
        "        # padding\n",
        "\n",
        "        self.X = X\n",
        "\n",
        "        Xp = self.padding.forward(X, self.kernel_size, self.s)\n",
        "\n",
        "        # convolve Xp with K\n",
        "        Z = self.convolve(Xp, self.K, self.s) + self.b\n",
        "\n",
        "        a = self.activation.forward(Z)\n",
        "\n",
        "        return a\n",
        "\n",
        "    def backpropagation(self, da):\n",
        "\n",
        "        Xp = self.padding.forward(self.X, self.kernel_size, self.s)\n",
        "\n",
        "        m, Nc, Nh, Nw = Xp.shape\n",
        "\n",
        "        dZ = self.activation.backpropagation(da)\n",
        "\n",
        "        # Dilate dZ (dZ-> dZ_D)\n",
        "\n",
        "        dZ_D = self.dilate2D(dZ, Dr=self.s)\n",
        "\n",
        "        dX = self.dZ_D_dX(dZ_D, Nh, Nw)\n",
        "\n",
        "        # Gradient dK\n",
        "\n",
        "        _, _, Hd, Wd = dZ_D.shape\n",
        "\n",
        "        ph = self.Nh - Hd - self.Kh + 1\n",
        "        pw = self.Nw - Wd - self.Kw + 1\n",
        "\n",
        "        padding_back = Padding2D(p=(ph, pw))\n",
        "\n",
        "        dZ_Dp = padding_back.forward(dZ_D, self.kernel_size, self.s)\n",
        "\n",
        "        self.dK = self.convolve(Xp, dZ_Dp, mode='param')\n",
        "\n",
        "        # Gradient db\n",
        "\n",
        "        self.db = np.sum(dZ, axis=0)\n",
        "\n",
        "        return dX\n",
        "\n",
        "    def update(self, lr, m, k):\n",
        "        '''\n",
        "        Parameters:\n",
        "\n",
        "        lr: learning rate\n",
        "        m: batch_size (sumber of samples in batch)\n",
        "        k: iteration_number\n",
        "        '''\n",
        "        dK, db = self.optimizer.get_optimization(self.dK, self.db, k)\n",
        "\n",
        "        if self.kernel_regularizer[0].lower()=='l2':\n",
        "            dK += self.kernel_regularizer[1] * self.K\n",
        "        elif self.weight_regularizer[0].lower()=='l1':\n",
        "            dK += self.kernel_regularizer[1] * np.sign(self.K)\n",
        "\n",
        "        self.K -= self.dK*(lr/m)\n",
        "\n",
        "        if self.use_bias:\n",
        "            self.b -= self.db*(lr/m)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d0d0fab-5f0e-49dd-a8be-a324ad038c69",
      "metadata": {
        "id": "0d0d0fab-5f0e-49dd-a8be-a324ad038c69"
      },
      "source": [
        "#### [Maxpool2D class](https://pythonandml.github.io/dlbook/content/convolutional_neural_networks/pooling_layers.html)\n",
        "\n",
        "This class will perform forward and backpropagation for a Pooling layer on a batch of 2D image with channels.\n",
        "\n",
        "Type of Pooling available are: `Max` and `Mean`\n",
        "\n",
        "![](https://github.com/pythonandml/dlbook/blob/master/content/convolutional_neural_networks/images/pooling_layer.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "60991fd8-1e00-4020-85c0-b05fb24bea35",
      "metadata": {
        "id": "60991fd8-1e00-4020-85c0-b05fb24bea35",
        "tags": [
          "hide-input"
        ]
      },
      "outputs": [],
      "source": [
        "class Pooling2D:\n",
        "\n",
        "    def __init__(self, pool_size=(2,2), s=(2,2), p='valid', pool_type='max'):\n",
        "        '''\n",
        "        Parameters:\n",
        "\n",
        "        pool_size: An integer or tuple/list of 2 integers,\n",
        "                     specifying the height and width of the 2D convolution window.\n",
        "\n",
        "        s: strides along height and width (sh, sw)\n",
        "\n",
        "        p: padding type\n",
        "           Allowed types are only 'same', 'valid', an integer or a tuple of length 2.\n",
        "\n",
        "        pool_type: pooling type\n",
        "        Allowed types are only 'max', or 'mean'\n",
        "        '''\n",
        "        self.padding = Padding2D(p=p)\n",
        "\n",
        "        if type(pool_size)==int:\n",
        "            self.pool_size = (pool_size, pool_size)\n",
        "        elif type(pool_size)==tuple and len(pool_size)==2:\n",
        "            self.pool_size = pool_size\n",
        "\n",
        "        self.Kh, self.Kw  = self.pool_size\n",
        "\n",
        "        if type(s)==int:\n",
        "            self.s = (s,s)\n",
        "        elif type(s)==tuple and len(s)==2:\n",
        "            self.s = s\n",
        "\n",
        "        self.sh, self.sw = self.s\n",
        "\n",
        "        self.pool_type = pool_type\n",
        "\n",
        "    def get_dimensions(self, input_shape):\n",
        "\n",
        "        if len(input_shape)==4:\n",
        "            m, Nc, Nh, Nw = input_shape\n",
        "        elif len(input_shape)==3:\n",
        "            Nc, Nh, Nw = input_shape\n",
        "\n",
        "        Oh = (Nh-self.Kh)//self.sh + 1\n",
        "        Ow = (Nw-self.Kw)//self.sw + 1\n",
        "\n",
        "        if len(input_shape)==4:\n",
        "            self.output_shape = (m, Nc, Oh, Ow)\n",
        "        elif len(input_shape)==3:\n",
        "            self.output_shape = (Nc, Oh, Ow)\n",
        "\n",
        "    def prepare_subMatrix(self, X, pool_size, s):\n",
        "        m, Nc, Nh, Nw = X.shape\n",
        "        sh, sw = s\n",
        "        Kh, Kw = pool_size\n",
        "\n",
        "        Oh = (Nh-Kh)//sh + 1\n",
        "        Ow = (Nw-Kw)//sw + 1\n",
        "\n",
        "        strides = (Nc*Nh*Nw, Nh*Nw, Nw*sh, sw, Nw, 1)\n",
        "        strides = tuple(i * X.itemsize for i in strides)\n",
        "\n",
        "        subM = np.lib.stride_tricks.as_strided(X,\n",
        "                                               shape=(m, Nc, Oh, Ow, Kh, Kw),\n",
        "                                               strides=strides)\n",
        "        return subM\n",
        "\n",
        "    def pooling(self, X, pool_size=(2,2), s=(2,2)):\n",
        "\n",
        "        subM = self.prepare_subMatrix(X, pool_size, s)\n",
        "\n",
        "        if self.pool_type=='max':\n",
        "            return np.max(subM, axis=(-2,-1))\n",
        "        elif self.pool_type=='mean':\n",
        "            return np.mean(subM, axis=(-2,-1))\n",
        "        else:\n",
        "            raise ValueError(\"Allowed pool types are only 'max' or 'mean'.\")\n",
        "\n",
        "    def prepare_mask(self, subM, Kh, Kw):\n",
        "\n",
        "        m, Nc, Oh, Ow, Kh, Kw = subM.shape\n",
        "\n",
        "        a = subM.reshape(-1,Kh*Kw)\n",
        "        idx = np.argmax(a, axis=1)\n",
        "        b = np.zeros(a.shape)\n",
        "        b[np.arange(b.shape[0]), idx] = 1\n",
        "        mask = b.reshape((m, Nc, Oh, Ow, Kh, Kw))\n",
        "\n",
        "        return mask\n",
        "\n",
        "    def mask_dXp(self, mask, Xp, dZ, Kh, Kw):\n",
        "        dA = np.einsum('i,ijk->ijk', dZ.reshape(-1), mask.reshape(-1,Kh,Kw)).reshape(mask.shape)\n",
        "        m, Nc, Nh, Nw = Xp.shape\n",
        "        strides = (Nc*Nh*Nw, Nh*Nw, Nw, 1)\n",
        "        strides = tuple(i * Xp.itemsize for i in strides)\n",
        "        dXp = np.lib.stride_tricks.as_strided(dA, Xp.shape, strides)\n",
        "        return dXp\n",
        "\n",
        "    def maxpool_backprop(self, dZ, X):\n",
        "\n",
        "        Xp = self.padding.forward(X, self.pool_size, self.s)\n",
        "\n",
        "        subM = self.prepare_subMatrix(Xp, self.pool_size, self.s)\n",
        "\n",
        "        m, Nc, Oh, Ow, Kh, Kw = subM.shape\n",
        "\n",
        "        m, Nc, Nh, Nw = Xp.shape\n",
        "\n",
        "        mask = self.prepare_mask(subM, Kh, Kw)\n",
        "\n",
        "        dXp = self.mask_dXp(mask, Xp, dZ, Kh, Kw)\n",
        "\n",
        "        return dXp\n",
        "\n",
        "    def dZ_dZp(self, dZ):\n",
        "        sh, sw = self.s\n",
        "        Kh, Kw = self.pool_size\n",
        "\n",
        "        dZp = np.kron(dZ, np.ones((Kh,Kw), dtype=dZ.dtype)) # similar to repelem in matlab\n",
        "\n",
        "        jh, jw = Kh-sh, Kw-sw # jump along height and width\n",
        "\n",
        "        if jw!=0:\n",
        "            L = dZp.shape[-1]-1\n",
        "\n",
        "            l1 = np.arange(sw, L)\n",
        "            l2 = np.arange(sw + jw, L + jw)\n",
        "\n",
        "            mask = np.tile([True]*jw + [False]*jw, len(l1)//jw).astype(bool)\n",
        "\n",
        "            r1 = l1[mask[:len(l1)]]\n",
        "            r2 = l2[mask[:len(l2)]]\n",
        "\n",
        "            dZp[:, :, :, r1] += dZp[:, :, :, r2]\n",
        "            dZp = np.delete(dZp, r2, axis=-1)\n",
        "\n",
        "        if jh!=0:\n",
        "            L = dZp.shape[-2]-1\n",
        "\n",
        "            l1 = np.arange(sh, L)\n",
        "            l2 = np.arange(sh + jh, L + jh)\n",
        "\n",
        "            mask = np.tile([True]*jh + [False]*jh, len(l1)//jh).astype(bool)\n",
        "\n",
        "            r1 = l1[mask[:len(l1)]]\n",
        "            r2 = l2[mask[:len(l2)]]\n",
        "\n",
        "            dZp[:, :, r1, :] += dZp[:, :, r2, :]\n",
        "            dZp = np.delete(dZp, r2, axis=-2)\n",
        "\n",
        "        return dZp\n",
        "\n",
        "    def averagepool_backprop(self, dZ, X):\n",
        "\n",
        "        Xp = self.padding.forward(X, self.pool_size, self.s)\n",
        "\n",
        "        m, Nc, Nh, Nw = Xp.shape\n",
        "\n",
        "        dZp = self.dZ_dZp(dZ)\n",
        "\n",
        "        ph = Nh - dZp.shape[-2]\n",
        "        pw = Nw - dZp.shape[-1]\n",
        "\n",
        "        padding_back = Padding2D(p=(ph, pw))\n",
        "\n",
        "        dXp = padding_back.forward(dZp, s=self.s, kernel_size=self.pool_size)\n",
        "\n",
        "        return dXp / (Nh*Nw)\n",
        "\n",
        "    def forward(self, X):\n",
        "        '''\n",
        "        Parameters:\n",
        "\n",
        "        X: input of shape (m, Nc, Nh, Nw)\n",
        "\n",
        "        Returns:\n",
        "\n",
        "        Z: pooled X\n",
        "        '''\n",
        "\n",
        "        self.X = X\n",
        "\n",
        "        # padding\n",
        "        Xp = self.padding.forward(X, self.pool_size, self.s)\n",
        "\n",
        "        Z = self.pooling(Xp, self.pool_size, self.s)\n",
        "\n",
        "        return Z\n",
        "\n",
        "    def backpropagation(self, dZ):\n",
        "        '''\n",
        "        Parameters:\n",
        "\n",
        "        dZ: Output Error\n",
        "\n",
        "        Return:\n",
        "\n",
        "        dX: Backprop Error of X\n",
        "        '''\n",
        "        if self.pool_type=='max':\n",
        "            dXp = self.maxpool_backprop(dZ, self.X)\n",
        "        elif self.pool_type=='mean':\n",
        "            dXp = self.averagepool_backprop(dZ, self.X)\n",
        "        dX = self.padding.backpropagation(dXp)\n",
        "        return dX"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17b3130f-b690-495f-93d6-70acf9440ea9",
      "metadata": {
        "id": "17b3130f-b690-495f-93d6-70acf9440ea9"
      },
      "source": [
        "#### Flatten class\n",
        "\n",
        "Converts 4D image $(m, N_c, N_h, N_w)$ to a 2D array of shape $(m, N_c \\times N_h \\times N_w)$ so that it can be sent to the Dense layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "d937d108-e4f5-4c96-8ae9-ad169f9bc6d9",
      "metadata": {
        "id": "d937d108-e4f5-4c96-8ae9-ad169f9bc6d9",
        "tags": [
          "hide-input"
        ]
      },
      "outputs": [],
      "source": [
        "class Flatten:\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.m, self.Nc, self.Nh, self.Nw = X.shape\n",
        "        X_flat = X.reshape((self.m, self.Nc * self.Nh * self.Nw))\n",
        "        return X_flat\n",
        "\n",
        "    def backpropagation(self, dZ):\n",
        "        dX = dZ.reshape((self.m, self.Nc, self.Nh, self.Nw))\n",
        "        return dX\n",
        "\n",
        "    def get_dimensions(self, input_shape):\n",
        "\n",
        "        if len(input_shape)==4:\n",
        "            self.m, self.Nc, self.Nh, self.Nw = input_shape\n",
        "        elif len(input_shape)==3:\n",
        "            self.Nc, self.Nh, self.Nw = input_shape\n",
        "\n",
        "        self.output_shape = self.Nc * self.Nh * self.Nw"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14923bea-70cc-452a-8c9e-80c336253328",
      "metadata": {
        "id": "14923bea-70cc-452a-8c9e-80c336253328"
      },
      "source": [
        "#### CNN\n",
        "\n",
        "This class finally contains the compile, summary, fit, predict, etc methods for executing our CNN model. Apart from `network_architecture`, `summary` and `initialize_parameters` all other functions are almost same to that of the functions used in [MLP model](https://pythonandml.github.io/dlbook/content/multilayer_perceptrons/neural_networks_mlp_scratch_best.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "c192ddf2-d5b4-452b-b4c2-5616a681ec1f",
      "metadata": {
        "id": "c192ddf2-d5b4-452b-b4c2-5616a681ec1f",
        "tags": [
          "hide-input"
        ]
      },
      "outputs": [],
      "source": [
        "class CNN:\n",
        "\n",
        "    def __init__(self, layers=None):\n",
        "        '''\n",
        "        This is a sequential CNN model\n",
        "        '''\n",
        "        if layers is None:\n",
        "            self.layers = []\n",
        "        else:\n",
        "            self.layers = layers\n",
        "        self.network_architecture_called = False\n",
        "\n",
        "    def add(self, layer):\n",
        "        # adds a layer to CNN model\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def Input(self, input_shape):\n",
        "        self.d = input_shape\n",
        "        self.architecture = [self.d] # output architecture\n",
        "        self.layer_name = [\"Input\"]\n",
        "\n",
        "    def network_architecture(self):\n",
        "        for layer in self.layers:\n",
        "            if layer.__class__.__name__=='Conv2D':\n",
        "                if layer.input_shape_x is not None:\n",
        "                    self.Input(layer.input_shape_x)\n",
        "                layer.get_dimensions(self.architecture[-1])\n",
        "                self.architecture.append(layer.output_shape)\n",
        "                self.layer_name.append(layer.__class__.__name__)\n",
        "            elif layer.__class__.__name__ in ['Flatten', 'Pooling2D']:\n",
        "                layer.get_dimensions(self.architecture[-1])\n",
        "                self.architecture.append(layer.output_shape)\n",
        "                self.layer_name.append(layer.__class__.__name__)\n",
        "            elif layer.__class__.__name__=='Dense':\n",
        "                self.architecture.append(layer.neurons)\n",
        "                self.layer_name.append(layer.__class__.__name__)\n",
        "            else:\n",
        "                self.architecture.append(self.architecture[-1])\n",
        "                self.layer_name.append(layer.__class__.__name__)\n",
        "\n",
        "        self.layers = [layer for layer in self.layers if layer is not None]\n",
        "        try:\n",
        "            idx = model.layer_name.index(\"NoneType\")\n",
        "            del model.layer_name[idx]\n",
        "            del model.architecture[idx]\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    def summary(self):\n",
        "        if self.network_architecture_called==False:\n",
        "            self.network_architecture()\n",
        "            self.network_architecture_called = True\n",
        "        len_assigned = [45, 26, 15]\n",
        "        count = {'Dense': 1, 'Activation': 1, 'Input': 1,\n",
        "                'BatchNormalization': 1, 'Dropout': 1, 'Conv2D': 1,\n",
        "                'Pooling2D': 1, 'Flatten': 1}\n",
        "\n",
        "        col_names = ['Layer (type)', 'Output Shape', '# of Parameters']\n",
        "\n",
        "        print(\"Model: CNN\")\n",
        "        print('-'*sum(len_assigned))\n",
        "\n",
        "        text = ''\n",
        "        for i in range(3):\n",
        "            text += col_names[i] + ' '*(len_assigned[i]-len(col_names[i]))\n",
        "        print(text)\n",
        "\n",
        "        print('='*sum(len_assigned))\n",
        "\n",
        "        total_params = 0\n",
        "        trainable_params = 0\n",
        "        non_trainable_params = 0\n",
        "\n",
        "        for i in range(len(self.layer_name)):\n",
        "            # layer name\n",
        "            layer_name = self.layer_name[i]\n",
        "            name = layer_name.lower() + '_' + str(count[layer_name]) + ' ' + '(' + layer_name + ')'\n",
        "            count[layer_name] += 1\n",
        "\n",
        "            # output shape\n",
        "            try:\n",
        "                out = '(None, '\n",
        "                for n in range(len(model.architecture[i])-1):\n",
        "                    out += str(model.architecture[i][n]) + ', '\n",
        "                out += str(model.architecture[i][-1]) + ')'\n",
        "            except:\n",
        "                out = '(None, ' + str(self.architecture[i]) + ')'\n",
        "\n",
        "            # number of params\n",
        "            if layer_name=='Dense':\n",
        "                h0 = self.architecture[i-1]\n",
        "                h1 = self.architecture[i]\n",
        "                if self.layers[i-1].use_bias:\n",
        "                    params = h0*h1 + h1\n",
        "                else:\n",
        "                    params = h0*h1\n",
        "                total_params += params\n",
        "                trainable_params += params\n",
        "            elif layer_name=='BatchNormalization':\n",
        "                h = self.architecture[i]\n",
        "                params = 4*h\n",
        "                trainable_params += 2*h\n",
        "                non_trainable_params += 2*h\n",
        "                total_params += params\n",
        "            elif layer_name=='Conv2D':\n",
        "                layer = self.layers[i-1]\n",
        "                if layer.use_bias:\n",
        "                    add_b = 1\n",
        "                else:\n",
        "                    add_b = 0\n",
        "                params = ((layer.Nc * layer.Kh * layer.Kw) + add_b) * layer.F\n",
        "                trainable_params += params\n",
        "                total_params += params\n",
        "            else:\n",
        "                # Pooling, Dropout, Flatten, Input\n",
        "                params = 0\n",
        "            names = [name, out, str(params)]\n",
        "\n",
        "            # print this row\n",
        "            text = ''\n",
        "            for j in range(3):\n",
        "                text += names[j] + ' '*(len_assigned[j]-len(names[j]))\n",
        "            print(text)\n",
        "            if i!=(len(self.layer_name)-1):\n",
        "                print('-'*sum(len_assigned))\n",
        "            else:\n",
        "                print('='*sum(len_assigned))\n",
        "\n",
        "        print(\"Total params:\", total_params)\n",
        "        print(\"Trainable params:\", trainable_params)\n",
        "        print(\"Non-trainable params:\", non_trainable_params)\n",
        "        print('-'*sum(len_assigned))\n",
        "\n",
        "    def compile(self, cost_type, optimizer_type):\n",
        "        self.cost = Cost(cost_type)\n",
        "        self.cost_type = cost_type\n",
        "        self.optimizer_type = optimizer_type\n",
        "\n",
        "    def initialize_parameters(self):\n",
        "        if self.network_architecture_called==False:\n",
        "            self.network_architecture()\n",
        "            self.network_architecture_called = True\n",
        "        # initialize parameters for different layers\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            if layer.__class__.__name__ in ['Dense', 'Conv2D']:\n",
        "                layer.initialize_parameters(self.architecture[i], self.optimizer_type)\n",
        "            elif layer.__class__.__name__=='BatchNormalization':\n",
        "                layer.initialize_parameters(self.architecture[i])\n",
        "\n",
        "    def fit(self, X, y, epochs=10, batch_size=5, lr=1, X_val=None, y_val=None, verbose=1, lr_decay=None, **kwargs):\n",
        "\n",
        "        self.history = {'Training Loss': [],\n",
        "                        'Validation Loss': [],\n",
        "                        'Training Accuracy': [],\n",
        "                        'Validation Accuracy': []}\n",
        "\n",
        "        iterations = 0\n",
        "        self.m = batch_size\n",
        "        self.initialize_parameters()\n",
        "\n",
        "        total_num_batches = np.ceil(len(X)/batch_size)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            cost_train = 0\n",
        "            num_batches = 0\n",
        "            y_pred_train = []\n",
        "            y_train = []\n",
        "\n",
        "            print('\\nEpoch: ' + str(epoch+1) + '/' + str(epochs))\n",
        "\n",
        "            for i in tqdm(range(0, len(X), batch_size)):\n",
        "                X_batch = X[i:i+batch_size]\n",
        "                y_batch = y[i:i+batch_size]\n",
        "\n",
        "                Z = X_batch.copy()\n",
        "\n",
        "                # feed-forward\n",
        "                for layer in self.layers:\n",
        "                    Z = layer.forward(Z)\n",
        "\n",
        "                # calculating training accuracy\n",
        "                if self.cost_type=='cross-entropy':\n",
        "                    y_pred_train += np.argmax(Z, axis=1).tolist()\n",
        "                    y_train += np.argmax(y_batch, axis=1).tolist()\n",
        "\n",
        "                # calculating the loss\n",
        "                cost_train += self.cost.get_cost(Z, y_batch) / self.m\n",
        "\n",
        "                # calculating dL/daL (last layer backprop error)\n",
        "                dZ = self.cost.get_d_cost(Z, y_batch)\n",
        "\n",
        "                # backpropagation\n",
        "                for layer in self.layers[::-1]:\n",
        "                    dZ = layer.backpropagation(dZ)\n",
        "\n",
        "                # Parameters update\n",
        "                for layer in self.layers:\n",
        "                    if layer.__class__.__name__ in ['Dense', 'BatchNormalization', 'Conv2D']:\n",
        "                        layer.update(lr, self.m, iterations)\n",
        "\n",
        "                # Learning rate decay\n",
        "                if lr_decay is not None:\n",
        "                    lr = lr_decay(iterations, **kwargs)\n",
        "\n",
        "                num_batches += 1\n",
        "                iterations += 1\n",
        "\n",
        "            cost_train /= num_batches\n",
        "\n",
        "            # printing purpose only (Training Accuracy, Validation loss and accuracy)\n",
        "\n",
        "            text  = 'Training Loss: ' + str(round(cost_train, 4)) + ' - '\n",
        "            self.history['Training Loss'].append(cost_train)\n",
        "\n",
        "            # training accuracy\n",
        "\n",
        "            if self.cost_type=='cross-entropy':\n",
        "                accuracy_train = np.sum(np.array(y_pred_train) == np.array(y_train)) / len(y_train)\n",
        "                text += 'Training Accuracy: ' + str(round(accuracy_train, 4))\n",
        "                self.history['Training Accuracy'].append(accuracy_train)\n",
        "            else:\n",
        "                text += 'Training Accuracy: ' + str(round(cost_train, 4))\n",
        "                self.history['Training Accuracy'].append(cost_train)\n",
        "\n",
        "            if X_val is not None:\n",
        "                cost_val, accuracy_val = self.evaluate(X_val, y_val, batch_size)\n",
        "                text += ' - Validation Loss: ' + str(round(cost_val, 4)) + ' - '\n",
        "                self.history['Validation Loss'].append(cost_val)\n",
        "                text += 'Validation Accuracy: ' + str(round(accuracy_val, 4))\n",
        "                self.history['Validation Accuracy'].append(accuracy_val)\n",
        "\n",
        "            if verbose:\n",
        "                print(text)\n",
        "            else:\n",
        "                print()\n",
        "\n",
        "    def evaluate(self, X, y, batch_size=None):\n",
        "\n",
        "        if batch_size is None:\n",
        "            batch_size = len(X)\n",
        "\n",
        "        cost = 0\n",
        "        correct = 0\n",
        "        num_batches = 0\n",
        "        utility = Utility()\n",
        "        Y_1hot, _ = utility.onehot(y)\n",
        "\n",
        "        for i in range(0, len(X), batch_size):\n",
        "            X_batch = X[i:i+batch_size]\n",
        "            y_batch = y[i:i+batch_size]\n",
        "            Y_1hot_batch = Y_1hot[i:i+batch_size]\n",
        "            Z = X_batch.copy()\n",
        "            for layer in self.layers:\n",
        "                if layer.__class__.__name__=='BatchNormalization':\n",
        "                    Z = layer.forward(Z, mode='test')\n",
        "                else:\n",
        "                    Z = layer.forward(Z)\n",
        "            if self.cost_type=='cross-entropy':\n",
        "                cost += self.cost.get_cost(Z, Y_1hot_batch) / len(y_batch)\n",
        "                y_pred = np.argmax(Z, axis=1).tolist()\n",
        "                correct += np.sum(y_pred == y_batch)\n",
        "            else:\n",
        "                cost += self.cost.get_cost(Z, y_batch) / len(y_batch)\n",
        "\n",
        "            num_batches += 1\n",
        "\n",
        "        if self.cost_type=='cross-entropy':\n",
        "            accuracy = correct / len(y)\n",
        "            cost /= num_batches\n",
        "            return cost, accuracy\n",
        "        else:\n",
        "            cost /= num_batches\n",
        "            return cost, cost\n",
        "\n",
        "    def loss_plot(self):\n",
        "        plt.plot(self.history['Training Loss'], 'k')\n",
        "        if len(self.history['Validation Loss'])>0:\n",
        "            plt.plot(self.history['Validation Loss'], 'r')\n",
        "            plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "            plt.title('Model Loss')\n",
        "        else:\n",
        "            plt.title('Training Loss')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.show()\n",
        "\n",
        "    def accuracy_plot(self):\n",
        "        plt.plot(self.history['Training Accuracy'], 'k')\n",
        "        if len(self.history['Validation Accuracy'])>0:\n",
        "            plt.plot(self.history['Validation Accuracy'], 'r')\n",
        "            plt.legend(['Train', 'Validation'], loc='lower right')\n",
        "            plt.title('Model Accuracy')\n",
        "        else:\n",
        "            plt.title('Training Accuracy')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.show()\n",
        "\n",
        "    def predict(self, X, batch_size=None):\n",
        "\n",
        "        if batch_size==None:\n",
        "            batch_size = len(X)\n",
        "\n",
        "        for i in range(0, len(X), batch_size):\n",
        "            X_batch = X[i:i+batch_size]\n",
        "            Z = X_batch.copy()\n",
        "            for layer in self.layers:\n",
        "                if layer.__class__.__name__=='BatchNormalization':\n",
        "                    Z = layer.forward(Z, mode='test')\n",
        "                else:\n",
        "                    Z = layer.forward(Z)\n",
        "            if i==0:\n",
        "                if self.cost_type=='cross-entropy':\n",
        "                    y_pred = np.argmax(Z, axis=1).tolist()\n",
        "                else:\n",
        "                    y_pred = Z\n",
        "            else:\n",
        "                if self.cost_type=='cross-entropy':\n",
        "                    y_pred += np.argmax(Z, axis=1).tolist()\n",
        "                else:\n",
        "                    y_pred = np.vstack((y_pred, Z))\n",
        "\n",
        "        return np.array(y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4fe9928-c7ae-4b5f-99cc-1f48506d2da4",
      "metadata": {
        "id": "f4fe9928-c7ae-4b5f-99cc-1f48506d2da4"
      },
      "source": [
        "### Validating model using MNIST Dataset\n",
        "\n",
        "Check this [page](https://en.wikipedia.org/wiki/MNIST_database) (link to an external website) to know more about **MNIST dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "04db9490-4476-43d6-857f-7a7422a01acc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04db9490-4476-43d6-857f-7a7422a01acc",
        "outputId": "2110bf65-e2b2-402a-e6d7-b3731ae95efb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((1, 28, 28), 10)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1], X_train.shape[2])\n",
        "X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1], X_test.shape[2])\n",
        "\n",
        "samples = 5000\n",
        "\n",
        "X_train = X_train[:samples, :]/255\n",
        "X_test = X_test[:samples, :]/255\n",
        "\n",
        "y_train = y_train[:samples]\n",
        "y_test = y_test[:samples]\n",
        "\n",
        "utility = Utility()\n",
        "\n",
        "# train validation split\n",
        "X_train_new, X_val, y_train_new, y_val = utility.train_test_split(X_train, y_train, test_ratio=0.2, seed=42)\n",
        "\n",
        "Y_1hot_train, _ = utility.onehot(y_train_new)\n",
        "\n",
        "input_shape = X_train_new.shape[1:]\n",
        "output_dim = Y_1hot_train.shape[1]\n",
        "\n",
        "input_shape, output_dim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "975c1435-bff8-42b9-aa9a-7454481670d5",
      "metadata": {
        "id": "975c1435-bff8-42b9-aa9a-7454481670d5"
      },
      "outputs": [],
      "source": [
        "model = CNN()\n",
        "\n",
        "model.add(model.Input(input_shape=input_shape))\n",
        "\n",
        "model.add(Conv2D(32, kernel_size=(5, 5), p='same', activation_type=\"relu\"))\n",
        "\n",
        "model.add(Pooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Dense(output_dim, activation_type=\"softmax\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "ZqvFjft3oQns",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqvFjft3oQns",
        "outputId": "64b00a53-1f3c-45d4-af46-67dc574a7655"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: CNN\n",
            "--------------------------------------------------------------------------------------\n",
            "Layer (type)                                 Output Shape              # of Parameters\n",
            "======================================================================================\n",
            "input_1 (Input)                              (None, 1, 28, 28)         0              \n",
            "--------------------------------------------------------------------------------------\n",
            "conv2d_1 (Conv2D)                            (None, 32, 28, 28)        832            \n",
            "--------------------------------------------------------------------------------------\n",
            "pooling2d_1 (Pooling2D)                      (None, 32, 14, 14)        0              \n",
            "--------------------------------------------------------------------------------------\n",
            "flatten_1 (Flatten)                          (None, 6272)              0              \n",
            "--------------------------------------------------------------------------------------\n",
            "dropout_1 (Dropout)                          (None, 6272)              0              \n",
            "--------------------------------------------------------------------------------------\n",
            "dense_1 (Dense)                              (None, 10)                62730          \n",
            "======================================================================================\n",
            "Total params: 63562\n",
            "Trainable params: 63562\n",
            "Non-trainable params: 0\n",
            "--------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "1b512e2d-023f-477f-a635-4cd27d71c4fd",
      "metadata": {
        "id": "1b512e2d-023f-477f-a635-4cd27d71c4fd"
      },
      "outputs": [],
      "source": [
        "batch_size = 256\n",
        "epochs = 10\n",
        "lr = 0.05\n",
        "\n",
        "model.compile(cost_type=\"cross-entropy\", optimizer_type=\"adam\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "nQLeDLye8C2S",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQLeDLye8C2S",
        "outputId": "49cb863f-d98a-4c2a-9414-58ab897e23af",
        "tags": [
          "hide-output"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [07:05<00:00, 26.57s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 2.2061 - Training Accuracy: 0.3025 - Validation Loss: 1.6472 - Validation Accuracy: 0.454\n",
            "\n",
            "Epoch: 2/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [06:55<00:00, 25.94s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 1.3552 - Training Accuracy: 0.535 - Validation Loss: 1.121 - Validation Accuracy: 0.648\n",
            "\n",
            "Epoch: 3/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [06:49<00:00, 25.58s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 1.0117 - Training Accuracy: 0.6577 - Validation Loss: 0.9004 - Validation Accuracy: 0.694\n",
            "\n",
            "Epoch: 4/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [06:40<00:00, 25.04s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.7985 - Training Accuracy: 0.727 - Validation Loss: 0.7702 - Validation Accuracy: 0.751\n",
            "\n",
            "Epoch: 5/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [06:31<00:00, 24.48s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.6912 - Training Accuracy: 0.766 - Validation Loss: 0.677 - Validation Accuracy: 0.77\n",
            "\n",
            "Epoch: 6/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [06:41<00:00, 25.10s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.6056 - Training Accuracy: 0.7988 - Validation Loss: 0.6384 - Validation Accuracy: 0.799\n",
            "\n",
            "Epoch: 7/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [07:48<00:00, 29.29s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.5665 - Training Accuracy: 0.81 - Validation Loss: 0.572 - Validation Accuracy: 0.806\n",
            "\n",
            "Epoch: 8/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [07:17<00:00, 27.32s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.5072 - Training Accuracy: 0.8355 - Validation Loss: 0.5284 - Validation Accuracy: 0.839\n",
            "\n",
            "Epoch: 9/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [08:10<00:00, 30.67s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.4754 - Training Accuracy: 0.8448 - Validation Loss: 0.5172 - Validation Accuracy: 0.843\n",
            "\n",
            "Epoch: 10/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [12:47<00:00, 48.00s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.4364 - Training Accuracy: 0.8615 - Validation Loss: 0.501 - Validation Accuracy: 0.853\n"
          ]
        }
      ],
      "source": [
        "LR_decay = LearningRateDecay()\n",
        "\n",
        "model.fit(X_train_new, Y_1hot_train, epochs=epochs, batch_size=batch_size, lr=lr, X_val=X_val,\n",
        "          y_val=y_val, verbose=1, lr_decay=LR_decay.constant, lr_0=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "RV9figNP8C2S",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "RV9figNP8C2S",
        "outputId": "45caaa59-71e7-4779-8cb1-789e86fb699f"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model.loss_plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "mRyGU_Wx8C2S",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "mRyGU_Wx8C2S",
        "outputId": "a2fc1ff2-bb65-4d5a-b3c2-7a450af6a94f"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model.accuracy_plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "nGCWRlUm8C2S",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGCWRlUm8C2S",
        "outputId": "4d7895d8-8570-46f4-8f04-83e797e727de"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[403,   0,   6,   5,   3,  10,  11,   1,  13,   8],\n",
              "       [  0, 547,   5,   3,   3,   3,   3,   1,   6,   0],\n",
              "       [  7,  14, 401,  36,   6,   7,   6,  15,  37,   1],\n",
              "       [  1,   6,  15, 387,   1,  37,   5,  20,  23,   5],\n",
              "       [  2,   2,   4,   0, 418,   2,   8,   8,   8,  48],\n",
              "       [ 12,   4,   6,  38,  10, 304,  11,  10,  51,  10],\n",
              "       [ 11,  10,  15,   1,  12,  22, 383,   1,   7,   0],\n",
              "       [  1,  10,  18,  12,  11,   1,   0, 415,   4,  40],\n",
              "       [ 10,   7,  13,  31,  20,  26,   2,  11, 350,  19],\n",
              "       [  2,   5,   4,   9,  47,   7,   2,  25,  11, 408]], dtype=int64)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_pred = model.predict(X_test, batch_size=batch_size)\n",
        "confusion_matrix(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "12VUatYM8C2T",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12VUatYM8C2T",
        "outputId": "4e22f817-4461-4173-e88f-51a23cc9da48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error Rate = 19.68\n",
            "Accuracy = 80.32\n"
          ]
        }
      ],
      "source": [
        "acc = accuracy_score(y_test, y_pred)\n",
        "print('Error Rate =',round((1-acc)*100, 2))\n",
        "print('Accuracy =',round((acc)*100, 2))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "py3.10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
